{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Multilingual Translation with M2M100 and Prompt Engineering**\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "**M2M100** is a sequence-to-sequence model developed by Facebook AI, designed specifically to handle multilingual translation tasks. Unlike traditional models that are often trained to translate between specific pairs of languages, M2M100 is trained to translate directly between any pair of 100 languages. This makes it a perfect choice for building a translation system that supports multiple languages, as it can handle translation between any two of the supported languages without needing intermediary translations.\n",
        "\n",
        "\n",
        "\n",
        "## **What is Prompt Engineering?**\n",
        "\n",
        "**Prompt Engineering** is the process of designing and optimizing input prompts to get better, more accurate, and context-aware outputs from AI models, especially large language models (LLMs) like GPT, mBART,M2M100 and BERT.\n",
        "\n",
        "\n",
        "## **Why Use Prompt Engineering in Language Translation?**\n",
        "🚀 Since M2M100 is a powerful pre-trained multilingual model, it can still sometimes produce imperfect translations, particularly when handling complex or context-heavy sentences. Prompt engineering helps by:\n",
        "\n",
        "✅ **Improving translation accuracy** by structuring input prompts in a way that the model understands better.\n",
        "\n",
        "✅ **Enhancing fluency** by including example translations (few-shot prompting) to guide the model.\n",
        "\n",
        "✅ **Explicit instructions** that provide clear guidelines, ensuring better results during translation.\n",
        "\n"
      ],
      "metadata": {
        "id": "s6A3q-U3957-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install indic-nlp-library nltk transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swAOEsGXIs1P",
        "outputId": "4d7840de-86c9-4868-bafd-fb0f3331e065"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Using cached indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.5)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries"
      ],
      "metadata": {
        "id": "bydQAtNfBMFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "tB-iaT2d_DFt"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **PyTorch** - Used for tensor operations and running deep learning models efficiently.\n",
        "* **M2M100Tokenizer** - Tokenizes input text and converts it into numerical representations that the model can process.\n",
        "* **M2M100ForConditionalGeneration** -  A multilingual transformer-based model from Facebook’s M2M-100 family, designed for high-quality sequence-to-sequence tasks like translation.\n"
      ],
      "metadata": {
        "id": "7QFo10lXBhin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK Punkt tokenizer models (if not already installed)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyRc-VUtKt1u",
        "outputId": "b1c338b4-5a45-49e6-e94f-fabea16c2116"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Pre-trained M2M100 Model and Tokenizer"
      ],
      "metadata": {
        "id": "c_v0XJ-HB7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model)\n"
      ],
      "metadata": {
        "id": "-8LWe0-LBeq7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Few-Shot Examples for Translation"
      ],
      "metadata": {
        "id": "9FpcrGM4CrYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "few_shot_examples = {\n",
        "    (\"en_XX\", \"mr_IN\"): [\n",
        "        \"Hello, how are you? -> नमस्कार, आपण कसे आहात?\",\n",
        "        \"Good morning! -> शुभ प्रभात!\",\n",
        "        \"What is your name? -> तुमचे नाव काय आहे?\",\n",
        "        \"My name is John -> माझं नाव जॉन आहे.\",\n",
        "        \"My name is John. Do not translate the name. -> माझं नाव जॉन आहे. नावाचे भाषांतर करू नका.\",\n",
        "        \"My name is Seema. Do not translate the name. -> माझं नाव सीमा आहे. नावाचे भाषांतर करू नका.\"\n",
        "    ],\n",
        "    (\"hi_IN\", \"en_XX\"): [\n",
        "        \"नमस्कार, आप कैसे हैं? -> Hello, how are you?\",\n",
        "        \"मुझे पढ़ाई पसंद है। -> I like studying.\",\n",
        "        \"आपका नाम क्या है? -> What is your name?\",\n",
        "        \"मेरा नाम सीमा है। -> My name is Seema.\",\n",
        "    ],\n",
        "    (\"mr_IN\", \"en_XX\"): [\n",
        "        \"तुमचे नाव काय आहे? -> What is your name?\",\n",
        "        \"माझे नाव रोहन आहे। -> My name is Rohan.\",\n",
        "        \"आज हवामान छान आहे। -> The weather is nice today.\",\n",
        "        \"माझे नाव सीमा आहे। -> My name is Seema.\",\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "HEJKC6evDrLr"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for Translation with Refined Prompt"
      ],
      "metadata": {
        "id": "gFF6WNQ2CyCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for translation with refined prompt\n",
        "def translate(text, source_lang, target_lang):\n",
        "    \"\"\"Translate text while avoiding unwanted instructions in the output.\"\"\"\n",
        "    # Retrieve few-shot examples if available\n",
        "    prompt_examples = few_shot_examples.get((source_lang, target_lang), [])\n",
        "\n",
        "    # Refined prompt with examples\n",
        "    prompt_text = \"\\n\".join(prompt_examples) + f\"\\n{text} \"\n",
        "\n",
        "    # Set source language\n",
        "    tokenizer.src_lang = source_lang\n",
        "\n",
        "    # Tokenize the input text and convert it into tensors\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate translated text\n",
        "    translated_tokens = model.generate(\n",
        "        **model_inputs,\n",
        "        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]  # Set target language\n",
        "    )\n",
        "\n",
        "    # Decode the translated output\n",
        "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the extra parts of the prompt, only keep the translation\n",
        "    translated_text = translated_text.split(\"\\n\")[-1].strip()\n",
        "\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "arN7lF5JCJwP"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Translate a Paragraph\n"
      ],
      "metadata": {
        "id": "u8N2sXEzC3e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate a paragraph\n",
        "def translate_paragraph(paragraph, source_lang, target_lang):\n",
        "    \"\"\"Translate the entire paragraph by first breaking it into sentences.\"\"\"\n",
        "\n",
        "    # Break the paragraph into sentences using NLTK's Punkt tokenizer\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "    # Translate each sentence and collect the translations\n",
        "    translated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = translate(sentence, source_lang, target_lang)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "\n",
        "    # Join the translated sentences to form a paragraph\n",
        "    translated_paragraph = ' '.join(translated_sentences)\n",
        "\n",
        "    return translated_paragraph\n"
      ],
      "metadata": {
        "id": "Dgv_EComEVAR"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "vb0pQHGGC6rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Hindi to English translation\n",
        "text1 = \"मुझे यात्रा करना पसंद है।\"\n",
        "translated_text3 = translate(text1, \"hi\", \"en\")\n",
        "print(f\"Hindi to English:\\n{text1} -> {translated_text3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnPmPfQEFjS8",
        "outputId": "947a3a63-e15e-4b7e-e28c-898bba1e7119"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi to English:\n",
            "मुझे यात्रा करना पसंद है। -> I like to travel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph translation (Marathi to English)\n",
        "paragraph_mr = \"\"\"नाव सोपे, लहान आणि अर्थपूर्ण असावे. उच्चारणासाठी कठीण, लांब किंवा खिल्ली उडवली जाणारी नावे टाळावीत. प्राचीन किंवा दुर्मिळ नावांच्या ऐवजी, काळास अनुसरून असलेले आणि कॉमन नसलेले नाव निवडावे. नावाचा अर्थ सकारात्मक आणि प्रेरणादायी असावा जेणेकरून बाळाचे व्यक्तिमत्त्व चांगले विकसीत होईल.\"\"\"\n",
        "translated_paragraph_mr = translate_paragraph(paragraph_mr, \"mr\", \"en\")\n",
        "print(f\"Translated Paragraph (Marathi to English):\\n{translated_paragraph_mr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tcb5FLjH_UK",
        "outputId": "2523c44a-6f28-44f8-801b-04b1457bc2a6"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Paragraph (Marathi to English):\n",
            "The name is simple, small and meaningful. Difficult, long or flushed for expression. The name of the ancient or ancient names, followed by the black and uncommon names. The name means positive and motivating rather than that the baby's personality will develop well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph translation (English to Marathi)\n",
        "paragraph_en = \"\"\"The Sun is the only star in our solar system. It is the center of our solar system, and its gravity holds the solar system together. Everything in our solar system revolves around it – the planets, asteroids, comets, and tiny bits of space debris.\"\"\"\n",
        "translated_paragraph_en = translate_paragraph(paragraph_en, \"en\", \"mr\")\n",
        "print(f\"Translated Paragraph (English to Marathi):\\n{translated_paragraph_en}\")\n"
      ],
      "metadata": {
        "id": "_nO3qP7aJn2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89295e88-4dd5-411a-d240-f54e6dd43d69"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Paragraph (English to Marathi):\n",
            "सूर्य आपल्या सौर प्रणालीत एकमात्र तारा आहे. हे आपल्या सौर प्रणालीचे केंद्र आहे आणि त्याचे गुरुत्वाकर्षण सौर प्रणाली एकत्र ठेवतो. आपल्या सौर प्रणालीतील सर्व गोष्टी त्यावर घरी जातात – ग्रह, एस्टेरॉयड, कॉमेट, आणि अंतरिक्षचे छोट्या बिट.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUK67YBb5vcI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}